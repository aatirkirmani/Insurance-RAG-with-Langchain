{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-header",
   "metadata": {},
   "source": [
    "# Insurance Documents RAG (Retrieval-Augmented Generation) System\n",
    "\n",
    "This notebook demonstrates a complete RAG implementation for processing and querying insurance policy documents. The system uses OpenAI embeddings, Chroma vector database, and cross-encoder reranking to provide accurate answers to insurance-related questions.\n",
    "\n",
    "## ðŸŽ¯ Objectives\n",
    "- Process PDF insurance documents\n",
    "- Create vector embeddings for semantic search\n",
    "- Implement advanced retrieval with reranking\n",
    "- Build a question-answering system using RAG architecture\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Document Loading and Processing](#document-loading)\n",
    "3. [Text Splitting and Chunking](#text-splitting)\n",
    "4. [Embeddings and Vector Database](#embeddings)\n",
    "5. [Retrieval System](#retrieval)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-setup",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### Installing Required Dependencies\n",
    "First, we install the necessary packages for our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-dependencies",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchainhub --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "Importing all necessary libraries for document processing, embeddings, vector storage, and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import openai\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain components\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore \n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker \n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RAG chain components\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-key-setup",
   "metadata": {},
   "source": [
    "### API Key Configuration\n",
    "Setting up OpenAI API key for accessing embeddings and chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "api-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_api_key_from_file(file_path):\n",
    "    \"\"\"Read API key from a text file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            api_key = file.read().strip()\n",
    "        return api_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load API key\n",
    "api_key = read_api_key_from_file('openai_key.txt')\n",
    "\n",
    "if api_key:\n",
    "    os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-initialization",
   "metadata": {},
   "source": [
    "### Initialize Language Model\n",
    "Setting up the OpenAI ChatGPT model for our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "llm-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "document-loading",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Document Loading and Processing\n",
    "\n",
    "### Load PDF Documents\n",
    "Loading insurance policy documents from the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "document-loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF documents from directory\n",
    "pdf_directory_loader = PyPDFDirectoryLoader('./InsuranceDocuments')\n",
    "documents = pdf_directory_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "document-analysis",
   "metadata": {},
   "source": [
    "### Document Analysis\n",
    "Analyzing the loaded documents to understand our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "analyze-docs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique document sources and filenames\n",
    "unique_sources = list(set(doc.metadata['source'] for doc in documents))\n",
    "filenames = [Path(source).name for source in sorted(unique_sources)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "display-files",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Motor Vehicle Insurance Policy Against Loss and Damage.pdf', 'Motor Vehicle Insurance Policy Against Third Party Liability.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "document-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34 document pages from 2 PDF files:\n",
      " 1. Motor Vehicle Insurance Policy Against Loss and Damage.pdf (16 pages)\n",
      " 2. Motor Vehicle Insurance Policy Against Third Party Liability.pdf (18 pages)\n"
     ]
    }
   ],
   "source": [
    "# Display document loading summary\n",
    "print(f\"Loaded {len(documents)} document pages from {len(unique_sources)} PDF files:\")\n",
    "for i, filename in enumerate(filenames, 1):\n",
    "    page_count = sum(1 for doc in documents if Path(doc.metadata['source']).name == filename)\n",
    "    print(f\"{i:2d}. {filename} ({page_count} pages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-splitting",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Text Splitting and Chunking\n",
    "\n",
    "### Configure Text Splitter\n",
    "Breaking down large documents into manageable chunks for better retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "text-splitter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure text splitter with optimal parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,         # Maximum characters per chunk\n",
    "    chunk_overlap=200,       # Overlap between chunks to maintain context\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking-analysis",
   "metadata": {},
   "source": [
    "### Chunking Analysis\n",
    "Analyzing the text splitting results to ensure optimal chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chunk-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 34 documents into 85 chunks\n",
      "Average chunks per document: 2.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Split {len(documents)} documents into {len(splits)} chunks\")\n",
    "print(f\"Average chunks per document: {len(splits) / len(documents):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chunk-size-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk sizes - Min: 112, Max: 998, Avg: 790\n"
     ]
    }
   ],
   "source": [
    "# Analyze chunk size distribution\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in splits]\n",
    "print(f\"Chunk sizes - Min: {min(chunk_sizes)}, Max: {max(chunk_sizes)}, Avg: {sum(chunk_sizes) / len(chunk_sizes):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunk-preview",
   "metadata": {},
   "source": [
    "### Preview Document Chunks\n",
    "Examining the first few chunks to understand the content structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "preview-chunks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick Text Preview:\n",
      "==================================================\n",
      "Chunk 1 from InsuranceDocuments\\Motor Vehicle Insurance Policy Against Loss and Damage.pdf (964 chars):\n",
      "Insurance Authority  \n",
      " \n",
      "The Unified Motor Vehicle Insurance Policy Against Loss and Damage  \n",
      "issued  pursuant to  the Regulation of Unified  Motor Vehicle Insurance Policies \n",
      "according to Insurance Au...\n",
      "----------------------------------------\n",
      "Chunk 2 from InsuranceDocuments\\Motor Vehicle Insurance Policy Against Loss and Damage.pdf (268 chars):\n",
      "the accident or was an injured party ; \n",
      " \n",
      "Therefore,  this Policy was entered into to cover the damages that befall on the \n",
      "Insured Motor Vehicle in the UAE during the insurance period according to th...\n",
      "----------------------------------------\n",
      "Chunk 3 from InsuranceDocuments\\Motor Vehicle Insurance Policy Against Loss and Damage.pdf (959 chars):\n",
      "Definitions:  \n",
      "The following terms and phrases shall have the meanings indicated beside  each  of \n",
      "them  unless the context provides otherwise:  \n",
      " \n",
      "Policy:  The Unified Motor Vehicle Insurance Policy ...\n",
      "----------------------------------------\n",
      "Chunk 4 from InsuranceDocuments\\Motor Vehicle Insurance Policy Against Loss and Damage.pdf (990 chars):\n",
      "Company, and paid or has agreed to pay the premium . \n",
      "Motor Vehicle Driver \n",
      "(Licensed Driver):  The insured or any person who drives the Motor \n",
      "Vehicle by the permission or order of the Insured, \n",
      "prov...\n",
      "----------------------------------------\n",
      "Chunk 5 from InsuranceDocuments\\Motor Vehicle Insurance Policy Against Loss and Damage.pdf (726 chars):\n",
      "Insurance Application:  The application that includes the details of the Insured, \n",
      "the details of the Motor Vehicle and the type of required  \n",
      "coverage, and is filled in by or with the knowledge of \n",
      "t...\n",
      "----------------------------------------\n",
      "Chunk 6 from InsuranceDocuments\\Motor Vehicle Insurance Policy Against Loss and Damage.pdf (989 chars):\n",
      "Natural Disaster:  Any general phenomenon that arises from nature such \n",
      "as floods, t ornados , hurricanes, volcanoes, earthquakes \n",
      "and quakes, and leads to extensive and widespread \n",
      "damage, and in res...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Quick Text Preview:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(min(6, len(splits))):\n",
    "    content = splits[i].page_content\n",
    "    source = splits[i].metadata.get('source', 'Unknown')\n",
    "    filename = source.split('/')[-1] if '/' in source else source\n",
    "    \n",
    "    print(f\"Chunk {i+1} from {filename} ({len(content)} chars):\")\n",
    "    print(content[:200] + \"...\" if len(content) > 200 else content)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Embeddings and Vector Database\n",
    "\n",
    "### Initialize Embedding Model\n",
    "Setting up OpenAI embeddings for converting text to vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "embedding-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\"  # OpenAI's recommended embedding model\n",
    ")\n",
    "\n",
    "# Test embeddings with sample text\n",
    "sample_texts = [splits[0].page_content]\n",
    "embeddings = embeddings_model.embed_documents(sample_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-testing",
   "metadata": {},
   "source": [
    "### Embedding Model Testing\n",
    "Verifying that embeddings are generated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "test-embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 1\n",
      "Embedding dimension: 1536\n",
      "Sample embedding (first 10 values): [0.0013451644002890914, -0.00765103255596719, -0.004609648689306264, -0.03582730583135431, -0.04965953248310294, 0.009724553854536255, -0.027087016532736485, -0.013635373926829458, 0.00594497017395262, -0.0019143985473220376]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"Sample embedding (first 10 values): {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "multiple-embeddings-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multiple embeddings created: 3 embeddings\n",
      "All embeddings have same dimension: True\n"
     ]
    }
   ],
   "source": [
    "# Test multiple embeddings\n",
    "if len(splits) >= 3:\n",
    "    multi_embeddings = embeddings_model.embed_documents([\n",
    "        splits[0].page_content,\n",
    "        splits[1].page_content,\n",
    "        splits[2].page_content\n",
    "    ])\n",
    "    print(f\"\\nMultiple embeddings created: {len(multi_embeddings)} embeddings\")\n",
    "    print(f\"All embeddings have same dimension: {all(len(emb) == len(multi_embeddings[0]) for emb in multi_embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-database-creation",
   "metadata": {},
   "source": [
    "### Create Vector Database\n",
    "Building a persistent Chroma vector database with cached embeddings for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "create-vectordb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977bc5bbaed64401836965f0814ee469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created with 85 document chunks\n",
      "Database persisted to: C:\\Users\\aatir\\OneDrive\\Documents\\upGrad_MLAI\\SemanticSpotter\\chroma_db\n",
      "Collection name: insurance_collection\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6853a45c7b5438aa70fdd55c87f55f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test query returned 2 similar documents\n"
     ]
    }
   ],
   "source": [
    "# Set up cached embeddings to avoid recomputation\n",
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "cache_store = InMemoryStore()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings=openai_embeddings,\n",
    "    document_embedding_cache=cache_store,\n",
    "    namespace=\"insurance_docs_embeddings\"\n",
    ")\n",
    "\n",
    "# Create persistent vector database\n",
    "persist_dir = \"./chroma_db\"\n",
    "db = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=cached_embeddings,\n",
    "    persist_directory=persist_dir,\n",
    "    collection_name=\"insurance_collection\"\n",
    ")\n",
    "\n",
    "# Display creation results\n",
    "print(f\"Vector database created with {len(splits)} document chunks\")\n",
    "print(f\"Database persisted to: {os.path.abspath(persist_dir)}\")\n",
    "print(f\"Collection name: insurance_collection\")\n",
    "\n",
    "# Test the database\n",
    "query_result = db.similarity_search(\"insurance policy\", k=2)\n",
    "print(f\"\\nTest query returned {len(query_result)} similar documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-similarity-search",
   "metadata": {},
   "source": [
    "### Basic Similarity Search Function\n",
    "Creating a simple search function for testing the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "simple-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query): \n",
    "    \"\"\"Perform basic similarity search on the vector database\"\"\"\n",
    "    return db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-search-function",
   "metadata": {},
   "source": [
    "### Enhanced Search Function with Formatting\n",
    "Creating a more sophisticated search function with formatted output for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "enhanced-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query, k=3):\n",
    "    \"\"\"Search for similar documents with pretty formatted output\"\"\"\n",
    "    results = db.similarity_search(query, k=k)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ðŸ” SEARCH QUERY: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Found {len(results)} relevant documents:\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        # Extract source file name\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        filename = Path(source).name if source != 'Unknown' else 'Unknown'\n",
    "        \n",
    "        # Get page number if available\n",
    "        page = doc.metadata.get('page', 'N/A')\n",
    "        \n",
    "        print(f\"ðŸ“„ RESULT {i}\")\n",
    "        print(f\"   Source: {filename}\")\n",
    "        print(f\"   Page: {page}\")\n",
    "        print(f\"   Content Length: {len(doc.page_content)} characters\")\n",
    "        print(f\"   Content Preview:\")\n",
    "        print(\"   \" + \"-\" * 60)\n",
    "        \n",
    "        content = doc.page_content.strip()\n",
    "        lines = content.split('\\n')\n",
    "        for line in lines[:100]:  # Show first 50 lines\n",
    "            print(f\"   {line}\")\n",
    "        \n",
    "        if len(lines) > 100:\n",
    "            print(f\"   ... ({len(lines) - 100} more lines)\")\n",
    "        \n",
    "        print(\"   \" + \"-\" * 60)\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-testing",
   "metadata": {},
   "source": [
    "### Testing Enhanced Search\n",
    "Testing the search functionality with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "test-enhanced-search",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b84d1105824a889a2a826d6ce514f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ” SEARCH QUERY: What happens if the motor vehicle becomes unroadworthy due to damage?\n",
      "================================================================================\n",
      "Found 1 relevant documents:\n",
      "\n",
      "ðŸ“„ RESULT 1\n",
      "   Source: Motor Vehicle Insurance Policy Against Loss and Damage.pdf\n",
      "   Page: 5\n",
      "   Content Length: 928 characters\n",
      "   Content Preview:\n",
      "   ------------------------------------------------------------\n",
      "   5. If the Insured Motor Vehicle is lost , proves to be irreparable , or that costs of \n",
      "   repair exceed 50% of the Motor Vehicle value before the accident, the  insured \n",
      "   value of the Motor Vehicle agreed upon between the Insurer and the Insured on \n",
      "   signing of the Insurance Policy will be the basis of calculation of the \n",
      "   compensation of loss and damage insured hereunder after deduction of the \n",
      "   Depreciation  Percentage of 20% f rom the insured value, and taking into account \n",
      "   the fraction of insurance period  (i.e., the proportion of  the period from the \n",
      "   commencement date of the insurance period to the date of the accident  to the \n",
      "   total insurance period) . \n",
      "   6. If the Motor Vehicle b ecomes unroadworthy  due to loss or damage insured \n",
      "   hereunder, the Company will bear the necessary costs of safeguarding and \n",
      "   transport ing the Motor Vehicle to the nearest repair shop , in order to  deliver it to \n",
      "   the Insured after repair.\n",
      "   ------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = similarity_search(\"What happens if the motor vehicle becomes unroadworthy due to damage?\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Advanced Retrieval System\n",
    "\n",
    "### Retriever with Cross-Encoder Reranking\n",
    "Implementing advanced retrieval with MMR (Maximal Marginal Relevance) and cross-encoder reranking for improved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5bb453d-f2cc-4951-bb65-c475c866b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(top_k=5):\n",
    "    \"\"\"Create retriever with reranking\"\"\"\n",
    "    # Base retriever with MMR\n",
    "    base_retriever = db.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": top_k, \"score_threshold\": 0.8})\n",
    "    \n",
    "    # Add reranker\n",
    "    cross_encoder = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "    reranker = CrossEncoderReranker(model=cross_encoder, top_n=top_k)\n",
    "    \n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=reranker,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "\n",
    "def search_documents(query, top_k=1):\n",
    "    \"\"\"Search and display results\"\"\"\n",
    "    retriever = create_retriever(top_k)\n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    print(f\"Found {len(docs)} documents for: '{query}'\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = Path(doc.metadata.get('source', 'Unknown')).name\n",
    "        page = doc.metadata.get('page', 'N/A')\n",
    "        \n",
    "        print(f\"Document {i} - {source} (Page {page})\")\n",
    "        print(f\"Content: {doc.page_content[:500]}...\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f185f2e1-c2b2-49a7-b369-290485956deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d51c641c244a8eae80d90f3779452a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 documents for: 'What are the Depreciation Percentages for Taxi Vehicles, Public Transport Vehicles and Rental Vehicles According to the Date of First Registration and Use'\n",
      "\n",
      "Document 1 - Motor Vehicle Insurance Policy Against Loss and Damage.pdf (Page 11)\n",
      "Content: Schedule No. (2)  \n",
      "Depreciation Percentages for Taxi  Vehicles , Public Transport Vehicles and Rental \n",
      "Vehicles According to the Date of First Registration and Use  \n",
      "Year  Percentage  \n",
      "Last si x months of the first year  10% \n",
      "Second  20% \n",
      "Third  25% \n",
      "Fourth  30% \n",
      "Fifth  35% \n",
      "Sixth  and above  40%...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "docs = search_documents(\"What are the Depreciation Percentages for Taxi Vehicles, Public Transport Vehicles and Rental Vehicles According to the Date of First Registration and Use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5640c68d-a45d-49f4-9425-1e52cc58182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\langchain\\hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    if not docs:\n",
    "        return \"No relevant documents found.\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs if doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef5d7bab-6e0e-41e5-856c-f26565ee660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_chain(retriever, llm_model=None, temperature=0):\n",
    "    \"\"\"Create a RAG chain with configurable components.\"\"\"\n",
    "    llm_model = llm_model or ChatOpenAI(temperature=temperature)\n",
    "    \n",
    "    return (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm_model\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdda1e69-8e17-456a-a402-6340513aaa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_rag_chain(create_retriever(top_k=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae51aa7f-4848-4ed3-bd89-ae870323a8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af617c99897f40cca51d60e88ebaf5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  response = response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The courts of the United Arab Emirates are competent to determine any disputes arising from this Policy.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which courts are competent to determine disputes from the Policy?\" \n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e196cce-833f-4c1c-a3d6-f43cb4885c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5701f201a4e40b9b15514372797295e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  response = response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A natural disaster is defined as any general phenomenon that arises from nature such as floods, tornadoes, hurricanes, volcanoes, earthquakes, and quakes, leading to extensive damage and requiring a decision from the concerned authority in the country. Floods are considered events within the concept of natural disasters. This definition is outlined in the policy regarding insurance coverage for such occurrences.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the definition of natural disaster as per the policy?\" \n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f35a8ac9-fdaf-493a-9cd6-7f70cd45e8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e3f0495323456fa48c00f023079432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  response = response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Third party liability is the liability for injuries and damages arising from the use of the insured motor vehicle to a third party or injured party. It covers bodily injury to a third party, either inside or outside the motor vehicle, and property damages to a third party. This type of liability does not cover accidents outside the borders of the state or those caused by natural disasters, warlike operations, or civil unrest.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which is a third party liability?\" \n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efde8dc4-e374-4a41-80f7-60fa4c7dff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357475cde6f44078896443aeadb96a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  response = response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The policy can be terminated before its expiration if there are serious grounds for termination during the policy period, with a notice sent to the insured thirty days prior to the fixed date of termination. The company must refund the paid premium after deducting a portion in proportion to the period the policy has remained in effect. Additionally, the policy can be terminated in case of a total loss to the motor vehicle, provided its registration is deleted with a report confirming it is unroadworthy.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"When can the policy be terminated before its expiration?\" \n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9015512-9365-4a12-a9f3-c902d21baf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
